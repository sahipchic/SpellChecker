{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "import re \n",
    "\n",
    "def is_russian(text):\n",
    "    ans = True\n",
    "    for i in text:\n",
    "        ans = ans & bool(re.search('[а-яА-Я]', i))\n",
    "    return bool(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "724512it [00:11, 63908.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "501381"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "words = {}\n",
    "\n",
    "with open('words.csv', newline='') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm(buffer):\n",
    "        if is_russian(r['Id']):\n",
    "            words[r['Id']] = int(r['Freq'])\n",
    "            \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362256it [00:12, 29217.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251385"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from Levenshtein import editops\n",
    "expected_train_values = set()\n",
    "train_values = list()\n",
    "map1 = {}\n",
    "map2 = {}\n",
    "map3 = {}\n",
    "replace_key = None\n",
    "with open('train.csv', newline='') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm(buffer):\n",
    "        if is_russian(r['Expected']):\n",
    "            expected_train_values.add(r['Expected'])\n",
    "            train_values.append((r['Id'], r['Expected']))\n",
    "            edits = editops(r['Id'], r['Expected'])\n",
    "            last_edit = None\n",
    "            letters = dict()\n",
    "            for i in r['Expected']:\n",
    "                letters[i] = 0\n",
    "            for i in r['Expected']:\n",
    "                letters[i] += 1\n",
    "\n",
    "            for edit in edits:\n",
    "                edit = list(edit)\n",
    "#                print(edit[0], edit[1])\n",
    "                if edit[0] == 'replace':\n",
    "                    replace_key = edit[0]\n",
    "                    edit[1] = r['Id'][edit[1]]\n",
    "                    edit[2] = r['Expected'][edit[2]]\n",
    "                    if letters.get(edit[2]) is not None:\n",
    "                        letters[edit[2]] -= 1\n",
    "                        if letters[edit[2]] == 0:\n",
    "                            del letters[edit[2]]\n",
    "                elif edit[0] == 'delete':\n",
    "                    edit[1] = r['Id'][edit[1]]\n",
    "                    edit[2] = None\n",
    "                elif edit[0] == 'insert':\n",
    "                    edit[1] = r['Expected'][edit[2]]\n",
    "                    if letters.get(edit[1]) is not None:\n",
    "                        letters[edit[1]] -= 1\n",
    "                        if letters[edit[1]] == 0:\n",
    "                            del letters[edit[1]]\n",
    "                    edit[2] = None\n",
    "\n",
    "                map1.setdefault(edit[0], 0)\n",
    "                map1[edit[0]] += 1\n",
    "                map2.setdefault(str(edit), 0)\n",
    "                map2[str(edit)] += 1\n",
    "                map3.setdefault((str(edit), last_edit), 0)\n",
    "                map3[(str(edit), last_edit)] += 1\n",
    "                last_edit = str(edit)\n",
    "            for i in letters:\n",
    "                cnt = letters[i]\n",
    "                map1.setdefault('replace', 0)\n",
    "                map1['replace'] += cnt\n",
    "                for j in range(cnt):\n",
    "                    l = str(['replace', i, i])\n",
    "                    map2.setdefault(l, 0)\n",
    "                    map2[l] += 1\n",
    "\n",
    "            \n",
    "train_values = [el for el, _ in groupby(train_values)]          \n",
    "len(train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362256it [00:01, 274613.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "with open('no_fix.submission.csv') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm(buffer):\n",
    "        train.append(r['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "from itertools import groupby\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.end = False\n",
    "        self.children = {}\n",
    "        self.prob = 0\n",
    "        self.word = None\n",
    "\n",
    "    def all_words(self, prefix):\n",
    "        if self.end:\n",
    "            yield prefix\n",
    "\n",
    "        for letter, child in self.children.items():\n",
    "            yield from child.all_words(prefix + letter)\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        self.nodes = []\n",
    "\n",
    "    def insert(self, word):\n",
    "        curr = self.root\n",
    "        for letter in word:\n",
    "            node = curr.children.get(letter)\n",
    "            if not node:\n",
    "                node = TrieNode()\n",
    "                curr.children[letter] = node\n",
    "            curr = node\n",
    "        curr.end = True\n",
    "        curr.prob = words[word] if words.get(word) is not None else 0\n",
    "        curr.word = word\n",
    "\n",
    "    def search(self, word):\n",
    "        curr = self.root\n",
    "        for letter in word:\n",
    "            node = curr.children.get(letter)\n",
    "            if not node:\n",
    "                return False\n",
    "            curr = node\n",
    "        return curr.end\n",
    "    \n",
    "    def all_words_beginning_with_prefix(self, prefix):\n",
    "        cur = self.root\n",
    "        for c in prefix:\n",
    "            cur = cur.children.get(c)\n",
    "            if cur is None:\n",
    "                return  # No words with given prefix\n",
    "\n",
    "        yield from cur.all_words(prefix)\n",
    "        \n",
    "    def dfs(self, word, node, prev_corr=None, pos=0, cor_num=0):\n",
    "        if pos == len(word):\n",
    "            if node.end:\n",
    "                self.nodes.append((node.prob, node.word))\n",
    "            return\n",
    "        if cor_num > 1:\n",
    "            if node.end and (pos == len(word)):\n",
    "                self.nodes.append((node.prob, node.word))\n",
    "            return\n",
    "        if len(node.children) == 0 and (pos == len(word)):\n",
    "            self.nodes.append((node.prob, node.word))\n",
    "            return\n",
    "        if node.children.get(word[pos]) is None:\n",
    "            if node.end and (pos == len(word)):\n",
    "                self.nodes.append((node.prob, node.word))\n",
    "            return\n",
    "        corrections = []\n",
    "        if pos >= len(word)-3:\n",
    "            corrections.append(('replace', word[pos], word[pos]))\n",
    "        else:\n",
    "            for ch in node.children:\n",
    "                corrections.append(('replace', word[pos], ch))\n",
    "            \n",
    "        p_cor = dict()\n",
    "        for c in corrections:\n",
    "            c = list(c)\n",
    "            p_cor.setdefault(str(c), 0)\n",
    "            p_cor[str(c)] = map2[str(c)] if map2.get(str(c)) is not None else 0\n",
    "        sorted_p_cor = OrderedDict(sorted(p_cor.items(), key=lambda kv: kv[1], reverse=True))\n",
    "        i = 0\n",
    "        for corr in sorted_p_cor:\n",
    "            corr = eval(corr)\n",
    "            if i > 3:\n",
    "                break\n",
    "            i += 1\n",
    "            if corr[0] == 'replace':\n",
    "                new_node = node.children[corr[2]]\n",
    "                if corr[1] == corr[2]:\n",
    "                    self.dfs(word, new_node, str(corr), pos + 1, cor_num)\n",
    "                else:\n",
    "                    self.dfs(word, new_node, str(corr), pos + 1, cor_num + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = PrefixTree()\n",
    "for word in (words.keys()):\n",
    "    trie.insert(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "3000\n",
      "5000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "12000\n",
      "13000\n",
      "15000\n",
      "17000\n",
      "19000\n",
      "21000\n",
      "23000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "31000\n",
      "34000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "48000\n",
      "49000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "56000\n",
      "57000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "70000\n",
      "71000\n",
      "73000\n",
      "74000\n",
      "76000\n",
      "77000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "87000\n",
      "89000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "95000\n",
      "99000\n",
      "101000\n",
      "103000\n",
      "105000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "124000\n",
      "125000\n",
      "128000\n",
      "129000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "140000\n",
      "141000\n",
      "143000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "156000\n",
      "157000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "168000\n",
      "169000\n",
      "172000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "181000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "205000\n",
      "206000\n",
      "208000\n",
      "210000\n",
      "212000\n",
      "214000\n",
      "215000\n",
      "218000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "236000\n",
      "237000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "253000\n",
      "254000\n",
      "256000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "264000\n",
      "265000\n",
      "267000\n",
      "268000\n",
      "273000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "280000\n",
      "282000\n",
      "283000\n",
      "285000\n",
      "287000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "313000\n",
      "316000\n",
      "318000\n",
      "322000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "341000\n",
      "343000\n",
      "344000\n",
      "347000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "0.051419438187359216\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "ans = []\n",
    "with open('no_fix.submission.csv') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    i = 0\n",
    "    sum_dist = 0\n",
    "    for r in (buffer):\n",
    "        if not is_russian(r['Id']):\n",
    "            ans.append((r['Id'], r['Id']))\n",
    "            sum_dist += distance(r['Predicted'], r['Id'])\n",
    "            i += 1\n",
    "            continue\n",
    "        trie.nodes = []\n",
    "        trie.dfs(r['Id'], trie.root)\n",
    "        mylist = list(dict.fromkeys(trie.nodes))\n",
    "        best_list = sorted(mylist, key = lambda x: x[0], reverse=True)\n",
    "        best_match = best_list[0][1] if len(best_list) > 1 else None\n",
    "        best_match2 = best_list[1][1] if len(best_list) > 1 else None\n",
    "        if best_match is None:\n",
    "            best_match = r['Id']\n",
    "        elif words[best_match] / words[r['Id']] < 5.0:\n",
    "            best_match = r['Id']\n",
    "        elif best_match2 is not None:\n",
    "            if words[best_match] / words[best_match2] < 3.0:\n",
    "                best_match = r['Id']\n",
    "        sum_dist += distance(r['Predicted'], best_match)\n",
    "        ans.append((r['Id'], best_match))\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    print(sum_dist / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_submission2.csv', 'w', newline='') as file:\n",
    "    buffer = csv.writer(file)\n",
    "    buffer.writerow(['Id', 'Predicted'])\n",
    "    for orig, pred in ans:\n",
    "        buffer.writerow([orig, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
